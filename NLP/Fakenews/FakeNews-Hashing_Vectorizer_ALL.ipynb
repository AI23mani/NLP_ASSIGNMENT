{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca66d99-6875-4d51-bd59-e10a834249e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db6cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 26381)\t-0.1336306209562122\n",
      "  (1, 46353)\t0.1336306209562122\n",
      "  (1, 76282)\t-0.1336306209562122\n",
      "  (1, 124604)\t-0.1336306209562122\n",
      "  (1, 271872)\t0.2672612419124244\n",
      "  (1, 354766)\t0.1336306209562122\n",
      "  (1, 355578)\t-0.1336306209562122\n",
      "  (1, 380136)\t-0.2672612419124244\n",
      "  (1, 399927)\t-0.1336306209562122\n",
      "  (1, 413315)\t-0.2672612419124244\n",
      "  (1, 421751)\t-0.2672612419124244\n",
      "  (1, 452780)\t-0.1336306209562122\n",
      "  (1, 506429)\t-0.5345224838248488\n",
      "  (1, 612563)\t-0.1336306209562122\n",
      "  (1, 615897)\t0.1336306209562122\n",
      "  (1, 626851)\t0.1336306209562122\n",
      "  (1, 639862)\t0.1336306209562122\n",
      "  (1, 691517)\t-0.1336306209562122\n",
      "  (1, 740856)\t-0.1336306209562122\n",
      "  (1, 777362)\t-0.2672612419124244\n",
      "  (1, 798576)\t-0.1336306209562122\n",
      "  (1, 907820)\t0.2672612419124244\n",
      "  (1, 1039472)\t-0.1336306209562122\n",
      "  (2, 14361)\t-0.3333333333333333\n",
      "  (2, 81229)\t0.3333333333333333\n",
      "  :\t:\n",
      "  (4243, 924171)\t0.08478501284163323\n",
      "  (4243, 934801)\t0.028261670947211076\n",
      "  (4243, 935153)\t0.028261670947211076\n",
      "  (4243, 935275)\t0.028261670947211076\n",
      "  (4243, 939307)\t-0.1130466837888443\n",
      "  (4243, 946139)\t-0.028261670947211076\n",
      "  (4243, 949760)\t0.028261670947211076\n",
      "  (4243, 953107)\t0.028261670947211076\n",
      "  (4243, 958756)\t-0.028261670947211076\n",
      "  (4243, 958992)\t-0.028261670947211076\n",
      "  (4243, 963256)\t-0.028261670947211076\n",
      "  (4243, 963865)\t-0.028261670947211076\n",
      "  (4243, 975215)\t0.028261670947211076\n",
      "  (4243, 982957)\t0.14130835473605538\n",
      "  (4243, 993015)\t0.028261670947211076\n",
      "  (4243, 1004174)\t0.028261670947211076\n",
      "  (4243, 1004879)\t-0.028261670947211076\n",
      "  (4243, 1009763)\t0.05652334189442215\n",
      "  (4243, 1010115)\t-0.028261670947211076\n",
      "  (4243, 1017691)\t-0.028261670947211076\n",
      "  (4243, 1026684)\t-0.028261670947211076\n",
      "  (4243, 1027460)\t-0.028261670947211076\n",
      "  (4243, 1038430)\t0.028261670947211076\n",
      "  (4243, 1039483)\t-0.028261670947211076\n",
      "  (4243, 1044795)\t0.05652334189442215\n",
      "Number of feature names: 1048576\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('news.csv', index_col=None)\n",
    "dataset=df.drop(\"Unnamed: 0\",axis=1)\n",
    "y=dataset[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size=0.33, random_state=53)\n",
    "Hashing_Vectorizer =HashingVectorizer(stop_words='english')\n",
    "Hashing_train =Hashing_Vectorizer.fit_transform(X_train)\n",
    "print(Hashing_train)\n",
    "Hashing_test =Hashing_Vectorizer.transform(X_test)\n",
    "feature_names = [f'feature_{i}' for i in range(Hashing_train.shape[1])]\n",
    "\n",
    "# Print number of feature names\n",
    "print(\"Number of feature names:\", len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79b12ca-0af9-45fd-ad1d-7c7375b084d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MANIKA\\Anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MANIKA\\Anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\MANIKA\\Anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15536\\1372560393.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;34m'Linear SVM'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlinear_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;34m'Kernel SVM'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkernel_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;34m'Naive Bayes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[1;34m'K-Nearest Neighbors'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk_nearest_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;34m'Decision Tree'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15536\\1372560393.py\u001b[0m in \u001b[0;36mnaive_bayes\u001b[1;34m(X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Fitting Naive Bayes to the Training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m    191\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    720\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    484\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                                       accept_large_sparse=accept_large_sparse)\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;31m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aiml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    289\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, cm\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test):\n",
    "    # Fitting Logistic Regression to the Training set\n",
    "    classifier = LogisticRegression(random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def linear_svm(X_train, y_train, X_test):\n",
    "    # Fitting Linear SVM to the Training set\n",
    "    classifier = SVC(kernel='linear', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def kernel_svm(X_train, y_train, X_test):\n",
    "    # Fitting Kernel SVM to the Training set\n",
    "    classifier = SVC(kernel='rbf', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def naive_bayes(X_train, y_train, X_test):\n",
    "    # Fitting Naive Bayes to the Training set\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def k_nearest_neighbors(X_train, y_train, X_test):\n",
    "    # Fitting K-Nearest Neighbors to the Training set\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def decision_tree(X_train, y_train, X_test):\n",
    "    # Fitting Decision Tree to the Training set\n",
    "    classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def random_forest(X_train, y_train, X_test):\n",
    "    # Fitting Random Forest to the Training set\n",
    "    classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "# Sample usage\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Apply HashingVectorizer\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "hash_train = hash_vectorizer.fit_transform(X_train)\n",
    "hash_test = hash_vectorizer.transform(X_test)\n",
    "\n",
    "# Call each algorithm function with the training and test data\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression(hash_train, y_train, hash_test),\n",
    "    'Linear SVM': linear_svm(hash_train, y_train, hash_test),\n",
    "    'Kernel SVM': kernel_svm(hash_train, y_train, hash_test),\n",
    "    'Naive Bayes': naive_bayes(hash_train, y_train, hash_test), \n",
    "    'K-Nearest Neighbors': k_nearest_neighbors(hash_train, y_train, hash_test),\n",
    "    'Decision Tree': decision_tree(hash_train, y_train, hash_test),\n",
    "    'Random Forest': random_forest(hash_train, y_train, hash_test)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for clf_name, (classifier, accuracy, report, cm) in models.items():\n",
    "    print(clf_name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadc3de-df77-4168-b407-9eaf9306defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize empty lists to store classifier names and accuracies\n",
    "classifier_names = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each classifier and store the accuracy\n",
    "for clf_name, (classifier, accuracy, _, _) in models.items():\n",
    "    classifier_names.append(clf_name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a DataFrame to store the accuracy scores\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Classifier': classifier_names,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(accuracy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc701f-163f-4ff0-b77c-b8d1287af7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store classifier names and accuracy scores\n",
    "classifiers = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each classifier and store the accuracy scores\n",
    "for clf_name, (_, accuracy, _, _) in models.items():\n",
    "    classifiers.append(clf_name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(classifiers, accuracies, color='skyblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Accuracy of Classifiers using TfidfVectorizerVectorizer')\n",
    "\n",
    "# Add accuracy scores to the end of each bar\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{acc:.2f}', \n",
    "             va='center', ha='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60a293-978e-4289-9d6f-ee65159a8c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

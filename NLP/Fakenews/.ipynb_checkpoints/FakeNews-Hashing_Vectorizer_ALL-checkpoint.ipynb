{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca66d99-6875-4d51-bd59-e10a834249e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551b99f5-b576-4031-9561-90f01d0a191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 26381)\t-0.1336306209562122\n",
      "  (1, 46353)\t0.1336306209562122\n",
      "  (1, 76282)\t-0.1336306209562122\n",
      "  (1, 124604)\t-0.1336306209562122\n",
      "  (1, 271872)\t0.2672612419124244\n",
      "  (1, 354766)\t0.1336306209562122\n",
      "  (1, 355578)\t-0.1336306209562122\n",
      "  (1, 380136)\t-0.2672612419124244\n",
      "  (1, 399927)\t-0.1336306209562122\n",
      "  (1, 413315)\t-0.2672612419124244\n",
      "  (1, 421751)\t-0.2672612419124244\n",
      "  (1, 452780)\t-0.1336306209562122\n",
      "  (1, 506429)\t-0.5345224838248488\n",
      "  (1, 612563)\t-0.1336306209562122\n",
      "  (1, 615897)\t0.1336306209562122\n",
      "  (1, 626851)\t0.1336306209562122\n",
      "  (1, 639862)\t0.1336306209562122\n",
      "  (1, 691517)\t-0.1336306209562122\n",
      "  (1, 740856)\t-0.1336306209562122\n",
      "  (1, 777362)\t-0.2672612419124244\n",
      "  (1, 798576)\t-0.1336306209562122\n",
      "  (1, 907820)\t0.2672612419124244\n",
      "  (1, 1039472)\t-0.1336306209562122\n",
      "  (2, 14361)\t-0.3333333333333333\n",
      "  (2, 81229)\t0.3333333333333333\n",
      "  :\t:\n",
      "  (4243, 924171)\t0.08478501284163323\n",
      "  (4243, 934801)\t0.028261670947211076\n",
      "  (4243, 935153)\t0.028261670947211076\n",
      "  (4243, 935275)\t0.028261670947211076\n",
      "  (4243, 939307)\t-0.1130466837888443\n",
      "  (4243, 946139)\t-0.028261670947211076\n",
      "  (4243, 949760)\t0.028261670947211076\n",
      "  (4243, 953107)\t0.028261670947211076\n",
      "  (4243, 958756)\t-0.028261670947211076\n",
      "  (4243, 958992)\t-0.028261670947211076\n",
      "  (4243, 963256)\t-0.028261670947211076\n",
      "  (4243, 963865)\t-0.028261670947211076\n",
      "  (4243, 975215)\t0.028261670947211076\n",
      "  (4243, 982957)\t0.14130835473605538\n",
      "  (4243, 993015)\t0.028261670947211076\n",
      "  (4243, 1004174)\t0.028261670947211076\n",
      "  (4243, 1004879)\t-0.028261670947211076\n",
      "  (4243, 1009763)\t0.05652334189442215\n",
      "  (4243, 1010115)\t-0.028261670947211076\n",
      "  (4243, 1017691)\t-0.028261670947211076\n",
      "  (4243, 1026684)\t-0.028261670947211076\n",
      "  (4243, 1027460)\t-0.028261670947211076\n",
      "  (4243, 1038430)\t0.028261670947211076\n",
      "  (4243, 1039483)\t-0.028261670947211076\n",
      "  (4243, 1044795)\t0.05652334189442215\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 33.2 GiB for an array with shape (4244, 1048576) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(Hashing_train)\n\u001b[0;32m      8\u001b[0m Hashing_test \u001b[38;5;241m=\u001b[39mHashing_Vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(Hashing_train\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1056\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1056\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\scipy\\sparse\\_base.py:1287\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 33.2 GiB for an array with shape (4244, 1048576) and data type float64"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('news.csv', index_col=None)\n",
    "dataset=df.drop(\"Unnamed: 0\",axis=1)\n",
    "y=dataset[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size=0.33, random_state=53)\n",
    "Hashing_Vectorizer =HashingVectorizer(stop_words='english')\n",
    "Hashing_train =Hashing_Vectorizer.fit_transform(X_train)\n",
    "print(Hashing_train)\n",
    "Hashing_test =Hashing_Vectorizer.transform(X_test)\n",
    "print(Hashing_train.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "173de866-e571-4b07-9443-dbb988100f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 26381)\t-0.1336306209562122\n",
      "  (1, 46353)\t0.1336306209562122\n",
      "  (1, 76282)\t-0.1336306209562122\n",
      "  (1, 124604)\t-0.1336306209562122\n",
      "  (1, 271872)\t0.2672612419124244\n",
      "  (1, 354766)\t0.1336306209562122\n",
      "  (1, 355578)\t-0.1336306209562122\n",
      "  (1, 380136)\t-0.2672612419124244\n",
      "  (1, 399927)\t-0.1336306209562122\n",
      "  (1, 413315)\t-0.2672612419124244\n",
      "  (1, 421751)\t-0.2672612419124244\n",
      "  (1, 452780)\t-0.1336306209562122\n",
      "  (1, 506429)\t-0.5345224838248488\n",
      "  (1, 612563)\t-0.1336306209562122\n",
      "  (1, 615897)\t0.1336306209562122\n",
      "  (1, 626851)\t0.1336306209562122\n",
      "  (1, 639862)\t0.1336306209562122\n",
      "  (1, 691517)\t-0.1336306209562122\n",
      "  (1, 740856)\t-0.1336306209562122\n",
      "  (1, 777362)\t-0.2672612419124244\n",
      "  (1, 798576)\t-0.1336306209562122\n",
      "  (1, 907820)\t0.2672612419124244\n",
      "  (1, 1039472)\t-0.1336306209562122\n",
      "  (2, 14361)\t-0.3333333333333333\n",
      "  (2, 81229)\t0.3333333333333333\n",
      "  :\t:\n",
      "  (4243, 924171)\t0.08478501284163323\n",
      "  (4243, 934801)\t0.028261670947211076\n",
      "  (4243, 935153)\t0.028261670947211076\n",
      "  (4243, 935275)\t0.028261670947211076\n",
      "  (4243, 939307)\t-0.1130466837888443\n",
      "  (4243, 946139)\t-0.028261670947211076\n",
      "  (4243, 949760)\t0.028261670947211076\n",
      "  (4243, 953107)\t0.028261670947211076\n",
      "  (4243, 958756)\t-0.028261670947211076\n",
      "  (4243, 958992)\t-0.028261670947211076\n",
      "  (4243, 963256)\t-0.028261670947211076\n",
      "  (4243, 963865)\t-0.028261670947211076\n",
      "  (4243, 975215)\t0.028261670947211076\n",
      "  (4243, 982957)\t0.14130835473605538\n",
      "  (4243, 993015)\t0.028261670947211076\n",
      "  (4243, 1004174)\t0.028261670947211076\n",
      "  (4243, 1004879)\t-0.028261670947211076\n",
      "  (4243, 1009763)\t0.05652334189442215\n",
      "  (4243, 1010115)\t-0.028261670947211076\n",
      "  (4243, 1017691)\t-0.028261670947211076\n",
      "  (4243, 1026684)\t-0.028261670947211076\n",
      "  (4243, 1027460)\t-0.028261670947211076\n",
      "  (4243, 1038430)\t0.028261670947211076\n",
      "  (4243, 1039483)\t-0.028261670947211076\n",
      "  (4243, 1044795)\t0.05652334189442215\n",
      "  (0, 22218)\t0.07930515857181442\n",
      "  (0, 30686)\t0.07930515857181442\n",
      "  (0, 33207)\t0.15861031714362883\n",
      "  (0, 34546)\t0.07930515857181442\n",
      "  (0, 49954)\t-0.07930515857181442\n",
      "  (0, 65592)\t0.07930515857181442\n",
      "  (0, 68450)\t0.07930515857181442\n",
      "  (0, 73301)\t-0.07930515857181442\n",
      "  (0, 84224)\t-0.07930515857181442\n",
      "  (0, 107719)\t-0.07930515857181442\n",
      "  (0, 109119)\t-0.07930515857181442\n",
      "  (0, 109449)\t-0.07930515857181442\n",
      "  (0, 116157)\t-0.15861031714362883\n",
      "  (0, 120741)\t0.07930515857181442\n",
      "  (0, 139615)\t0.07930515857181442\n",
      "  (0, 168762)\t0.07930515857181442\n",
      "  (0, 184737)\t0.07930515857181442\n",
      "  (0, 195890)\t0.07930515857181442\n",
      "  (0, 199353)\t-0.07930515857181442\n",
      "  (0, 199729)\t0.15861031714362883\n",
      "  (0, 201217)\t-0.07930515857181442\n",
      "  (0, 205672)\t-0.07930515857181442\n",
      "  (0, 223976)\t0.07930515857181442\n",
      "  (0, 231399)\t0.07930515857181442\n",
      "  (0, 231559)\t0.07930515857181442\n",
      "  :\t:\n",
      "  (2090, 974496)\t0.028952080535072162\n",
      "  (2090, 975831)\t-0.028952080535072162\n",
      "  (2090, 978262)\t-0.028952080535072162\n",
      "  (2090, 978892)\t-0.028952080535072162\n",
      "  (2090, 982957)\t0.17371248321043298\n",
      "  (2090, 986287)\t0.028952080535072162\n",
      "  (2090, 997384)\t-0.028952080535072162\n",
      "  (2090, 999854)\t0.057904161070144324\n",
      "  (2090, 1002026)\t-0.028952080535072162\n",
      "  (2090, 1004879)\t-0.028952080535072162\n",
      "  (2090, 1011185)\t-0.057904161070144324\n",
      "  (2090, 1011219)\t-0.028952080535072162\n",
      "  (2090, 1011472)\t-0.028952080535072162\n",
      "  (2090, 1012991)\t-0.028952080535072162\n",
      "  (2090, 1013040)\t0.028952080535072162\n",
      "  (2090, 1018737)\t0.028952080535072162\n",
      "  (2090, 1019904)\t-0.028952080535072162\n",
      "  (2090, 1021705)\t0.028952080535072162\n",
      "  (2090, 1024825)\t-0.057904161070144324\n",
      "  (2090, 1031868)\t-0.028952080535072162\n",
      "  (2090, 1034356)\t0.028952080535072162\n",
      "  (2090, 1035167)\t0.057904161070144324\n",
      "  (2090, 1036540)\t0.028952080535072162\n",
      "  (2090, 1039093)\t-0.028952080535072162\n",
      "  (2090, 1044795)\t0.028952080535072162\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('news.csv', index_col=None)\n",
    "dataset = df.drop(\"Unnamed: 0\", axis=1)\n",
    "y = dataset[\"label\"]\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Apply HashingVectorizer\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "hash_train = hash_vectorizer.fit_transform(X_train)\n",
    "print(hash_train)\n",
    "\n",
    "hash_test = hash_vectorizer.transform(X_test)\n",
    "print(hash_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79b12ca-0af9-45fd-ad1d-7c7375b084d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 88\u001b[0m\n\u001b[0;32m     81\u001b[0m hash_test \u001b[38;5;241m=\u001b[39m hash_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Call each algorithm function with the training and test data\u001b[39;00m\n\u001b[0;32m     84\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: logistic_regression(hash_train, y_train, hash_test),\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear SVM\u001b[39m\u001b[38;5;124m'\u001b[39m: linear_svm(hash_train, y_train, hash_test),\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKernel SVM\u001b[39m\u001b[38;5;124m'\u001b[39m: kernel_svm(hash_train, y_train, hash_test),\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaive Bayes\u001b[39m\u001b[38;5;124m'\u001b[39m: naive_bayes(hash_train, y_train, hash_test), \n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK-Nearest Neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: k_nearest_neighbors(hash_train, y_train, hash_test),\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m'\u001b[39m: decision_tree(hash_train, y_train, hash_test),\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: random_forest(hash_train, y_train, hash_test)\n\u001b[0;32m     92\u001b[0m }\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_name, (classifier, accuracy, report, cm) \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[13], line 50\u001b[0m, in \u001b[0;36mnaive_bayes\u001b[1;34m(X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnaive_bayes\u001b[39m(X_train, y_train, X_test):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Fitting Naive Bayes to the Training set\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[1;32m---> 50\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     51\u001b[0m     accuracy, report, cm \u001b[38;5;241m=\u001b[39m evaluate_classifier(classifier, X_test, y_test)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier, accuracy, report, cm\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\sklearn\\naive_bayes.py:263\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_fit(\n\u001b[0;32m    264\u001b[0m     X, y, np\u001b[38;5;241m.\u001b[39munique(y), _refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[0;32m    265\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\sklearn\\naive_bayes.py:423\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    422\u001b[0m first_call \u001b[38;5;241m=\u001b[39m _check_partial_fit_first_call(\u001b[38;5;28mself\u001b[39m, classes)\n\u001b[1;32m--> 423\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39mfirst_call)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    425\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\sklearn\\utils\\validation.py:1192\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1187\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     )\n\u001b[1;32m-> 1192\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1193\u001b[0m     X,\n\u001b[0;32m   1194\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1195\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1196\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1197\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1198\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1199\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1200\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1201\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1202\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1203\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1204\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1205\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1206\u001b[0m )\n\u001b[0;32m   1208\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1210\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[0;32m    916\u001b[0m     _ensure_no_complex_data(array)\n\u001b[1;32m--> 917\u001b[0m     array \u001b[38;5;241m=\u001b[39m _ensure_sparse_format(\n\u001b[0;32m    918\u001b[0m         array,\n\u001b[0;32m    919\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m    920\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    921\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    922\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m    923\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m    924\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    926\u001b[0m     )\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\iml\\Lib\\site-packages\\sklearn\\utils\\validation.py:557\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(sparse_container, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accept_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    556\u001b[0m     padded_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m input_name \u001b[38;5;28;01mif\u001b[39;00m input_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse data was passed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadded_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but dense data is required. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.toarray()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to convert to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(accept_sparse, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accept_sparse) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, cm\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test):\n",
    "    # Fitting Logistic Regression to the Training set\n",
    "    classifier = LogisticRegression(random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def linear_svm(X_train, y_train, X_test):\n",
    "    # Fitting Linear SVM to the Training set\n",
    "    classifier = SVC(kernel='linear', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def kernel_svm(X_train, y_train, X_test):\n",
    "    # Fitting Kernel SVM to the Training set\n",
    "    classifier = SVC(kernel='rbf', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def naive_bayes(X_train, y_train, X_test):\n",
    "    # Fitting Naive Bayes to the Training set\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def k_nearest_neighbors(X_train, y_train, X_test):\n",
    "    # Fitting K-Nearest Neighbors to the Training set\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def decision_tree(X_train, y_train, X_test):\n",
    "    # Fitting Decision Tree to the Training set\n",
    "    classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "def random_forest(X_train, y_train, X_test):\n",
    "    # Fitting Random Forest to the Training set\n",
    "    classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy, report, cm = evaluate_classifier(classifier, X_test, y_test)\n",
    "    return classifier, accuracy, report, cm\n",
    "\n",
    "# Sample usage\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Apply HashingVectorizer\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "hash_train = hash_vectorizer.fit_transform(X_train)\n",
    "hash_test = hash_vectorizer.transform(X_test)\n",
    "\n",
    "# Call each algorithm function with the training and test data\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression(hash_train, y_train, hash_test),\n",
    "    'Linear SVM': linear_svm(hash_train, y_train, hash_test),\n",
    "    'Kernel SVM': kernel_svm(hash_train, y_train, hash_test),\n",
    "    'Naive Bayes': naive_bayes(hash_train, y_train, hash_test), \n",
    "    'K-Nearest Neighbors': k_nearest_neighbors(hash_train, y_train, hash_test),\n",
    "    'Decision Tree': decision_tree(hash_train, y_train, hash_test),\n",
    "    'Random Forest': random_forest(hash_train, y_train, hash_test)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for clf_name, (classifier, accuracy, report, cm) in models.items():\n",
    "    print(clf_name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadc3de-df77-4168-b407-9eaf9306defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize empty lists to store classifier names and accuracies\n",
    "classifier_names = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each classifier and store the accuracy\n",
    "for clf_name, (classifier, accuracy, _, _) in models.items():\n",
    "    classifier_names.append(clf_name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a DataFrame to store the accuracy scores\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Classifier': classifier_names,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(accuracy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc701f-163f-4ff0-b77c-b8d1287af7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store classifier names and accuracy scores\n",
    "classifiers = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each classifier and store the accuracy scores\n",
    "for clf_name, (_, accuracy, _, _) in models.items():\n",
    "    classifiers.append(clf_name)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(classifiers, accuracies, color='skyblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Accuracy of Classifiers using TfidfVectorizerVectorizer')\n",
    "\n",
    "# Add accuracy scores to the end of each bar\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{acc:.2f}', \n",
    "             va='center', ha='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60a293-978e-4289-9d6f-ee65159a8c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
